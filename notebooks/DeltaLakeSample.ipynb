{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sample retrive delta tables from s3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Skip if you plan to use Spark to retrieve the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%sh\n",
        "pip3 install awswrangler "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Imports and configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "import boto3\n",
        "from delta.tables import DeltaTable\n",
        "from pyspark import SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "#################################################\n",
        "### conf spark context and install delta-core ###\n",
        "#################################################\n",
        "conf = SparkConf()\n",
        "conf.set('spark.jars.packages', 'io.delta:delta-core_2.12:1.0.0')\n",
        "conf.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
        "conf.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
        "\n",
        "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
        "\n",
        "###########################################\n",
        "### Set aws credentials (From env_vars) ###\n",
        "###########################################\n",
        "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.DefaultAWSCredentialsProviderChain\")\n",
        "\n",
        "### Init session aws ###\n",
        "session = boto3.Session()\n",
        "s3_resource = session.resource('s3')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "s3_path_in = f\"s3://bucketname/uri/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cols_use = [\n",
        "    'field1',\n",
        "    'field2',\n",
        "    'field3'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Spark mode\n",
        "df = spark.read.parquet(s3_path_in).select(cols_use)\n",
        "\n",
        "### pandas mode (if awsrangles was installed)\n",
        "### use_threads=True (Max threads available use)\n",
        "df = wr.s3.read_parquet(s3_path_in, dataset=True, boto3_session=session, columns=cols_use, use_threads=6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### view spark dataframe\n",
        "df.show(10)\n",
        "\n",
        "### view pandas dataframe\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example of creating and upsert delta tables from s3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "s3_bucket = \"data-test-bucket\"\n",
        "s3_path = f\"s3://bucketname/uri/\"\n",
        "\n",
        "### initially prepopulate the table with some data\n",
        "users_initial = [\n",
        "    { 'user_id': 1, 'name': 'Gina Burch', 'gender': 'f' },\n",
        "    { 'user_id': 2, 'name': 'Francesco Coates', 'gender': 'm' },\n",
        "    { 'user_id': 3, 'name': 'Saeed Wicks', 'gender': 'm' },\n",
        "    { 'user_id': 4, 'name': 'Raisa Oconnell', 'gender': 'f' },\n",
        "    { 'user_id': 5, 'name': 'Josh Copeland', 'gender': 'm' },\n",
        "    { 'user_id': 6, 'name': 'Kaiden Williamson', 'gender': 'm' }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "### Create df and load into s3\n",
        "spark.createDataFrame(users_initial) \\\n",
        "  .write.format(\"delta\").mode(\"overwrite\").save(s3_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "### load results via Spark API\n",
        "print(\"DF after initial load:\")\n",
        "spark.read.format(\"delta\").load(s3_path).orderBy(\"user_id\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "### new data\n",
        "users_append = [\n",
        "        { 'user_id': 6, 'name': 'Kaiden Mccarty', 'gender': 'm' },\n",
        "        { 'user_id': 7, 'name': 'Melody Gamble', 'gender': 'f' },\n",
        "        { 'user_id': 8, 'name': 'Alexandre Huff', 'gender': 'm' },\n",
        "]\n",
        "\n",
        "df_users_append = spark.createDataFrame(users_append)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "### Upsert delta mode\n",
        "deltaTable = DeltaTable.forPath(spark, s3_path)\n",
        "\n",
        "deltaTable.alias(\"old\") \\\n",
        "  .merge( df_users_append.alias(\"new\"), \"old.user_id = new.user_id\" ) \\\n",
        "  .whenMatchedUpdateAll() \\\n",
        "  .whenNotMatchedInsertAll() \\\n",
        "  .execute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "### load results via DeltaTable\n",
        "print(\"\\nDF after upserting data:\")\n",
        "deltaTable.toDF().orderBy(\"user_id\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "### list all files before vacuum\n",
        "bucket = boto3.resource(\"s3\").Bucket(s3_bucket)\n",
        "print(\"\\nObjects on S3 level BEFORE vacuum:\")\n",
        "[print(obj.key) for obj in bucket.objects.all()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "### list all files after vacuum\n",
        "print(\"\\nObjects on S3 level AFTER vacuum:\")\n",
        "[print(obj.key) for obj in bucket.objects.all()]"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    },
    "name": "python"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
